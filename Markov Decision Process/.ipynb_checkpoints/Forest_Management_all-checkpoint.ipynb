{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "willing-earthquake",
   "metadata": {},
   "source": [
    "# Module loading and functions defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "selective-invasion",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hiive.mdptoolbox.mdp import ValueIteration, PolicyIteration, QLearning\n",
    "import hiive.mdptoolbox.example as mdpex\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import sys\n",
    "import gym\n",
    "import os\n",
    "from numpy.random import choice\n",
    "\n",
    "np.random.seed(56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "romantic-relay",
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
    "\n",
    "def test_policy(P, R, policy, test_count=1000, gamma=0.9):\n",
    "    num_state = P.shape[-1]\n",
    "    total_episode = num_state * test_count\n",
    "    # start in each state\n",
    "    total_reward = 0\n",
    "    for state in range(num_state):\n",
    "        state_reward = 0\n",
    "        for state_episode in range(test_count):\n",
    "            episode_reward = 0\n",
    "            disc_rate = 1\n",
    "            while True:\n",
    "                # take step\n",
    "                action = policy[state]\n",
    "                # get next step using P\n",
    "                probs = P[action][state]\n",
    "                candidates = list(range(len(P[action][state])))\n",
    "                next_state =  choice(candidates, 1, p=probs)[0]\n",
    "                # get the reward\n",
    "                reward = R[state][action] * disc_rate\n",
    "                episode_reward += reward\n",
    "                # when go back to 0 ended\n",
    "                disc_rate *= gamma\n",
    "                if next_state == 0:\n",
    "                    break\n",
    "            state_reward += episode_reward\n",
    "        total_reward += state_reward\n",
    "    return total_reward / total_episode\n",
    "\n",
    "def trainVI(P, R, discount=0.9, epsilon=[1e-9]):\n",
    "    vi_df = pd.DataFrame(columns=[\"Epsilon\", \"Policy\", \"Iteration\", \n",
    "                                  \"Time\", \"Reward\", \"Value Function\"])\n",
    "    for eps in epsilon:\n",
    "        vi = ValueIteration(P, R, gamma=discount, epsilon=eps, max_iter=int(1e15))\n",
    "        vi.run()\n",
    "        reward = test_policy(P, R, vi.policy)\n",
    "        info = [float(eps), vi.policy, vi.iter, vi.time, reward, vi.V]\n",
    "        df_length = len(vi_df)\n",
    "        vi_df.loc[df_length] = info\n",
    "    return vi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "anticipated-procedure",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainQ(P, R, discount=0.9, alpha_dec=[0.99], alpha_min=[0.001], \n",
    "            epsilon=[1.0], epsilon_decay=[0.99], n_iter=[1000000]):\n",
    "    q_df = pd.DataFrame(columns=[\"Iterations\", \"Alpha Decay\", \"Alpha Min\", \n",
    "                                 \"Epsilon\", \"Epsilon Decay\", \"Reward\",\n",
    "                                 \"Time\", \"Policy\", \"Value Function\",\n",
    "                                 \"Training Rewards\"])\n",
    "    \n",
    "    count = 0\n",
    "    for i in n_iter:\n",
    "        for eps in epsilon:\n",
    "            for eps_dec in epsilon_decay:\n",
    "                for a_dec in alpha_dec:\n",
    "                    for a_min in alpha_min:\n",
    "                        q = QLearning(P, R, discount, alpha_decay=a_dec, \n",
    "                                      alpha_min=a_min, epsilon=eps, \n",
    "                                      epsilon_decay=eps_dec, n_iter=i)\n",
    "                        q.run()\n",
    "                        reward = test_policy(P, R, q.policy)\n",
    "                        count += 1\n",
    "                        print(\"{}: {}\".format(count, reward))\n",
    "                        st = q.run_stats\n",
    "                        rews = [s['Reward'] for s in st]\n",
    "                        info = [i, a_dec, a_min, eps, eps_dec, reward, \n",
    "                                q.time, q.policy, q.V, rews]\n",
    "                        \n",
    "                        df_length = len(q_df)\n",
    "                        q_df.loc[df_length] = info\n",
    "    return q_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "necessary-innocent",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pi(P, R, gamma = [0.9], max_iter = [1e6]):\n",
    "    result = {}\n",
    "    for g in gamma:\n",
    "        result[g] = {}\n",
    "        for itr in max_iter:\n",
    "            result[g][itr] = {}\n",
    "            pi = PolicyIteration(P, R, gamma=g, max_iter=itr)\n",
    "            pi.run()\n",
    "            pi_pol = pi.policy\n",
    "            pi_reward = test_policy(P, R, pi_pol)\n",
    "            pi_iter = pi.iter\n",
    "            pi_time = pi.time\n",
    "            result[g][itr][\"policy\"] = pi_pol\n",
    "            result[g][itr][\"reward\"] = pi_reward\n",
    "            result[g][itr][\"iteration\"] = pi_iter\n",
    "            result[g][itr][\"time\"] = pi_time\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "large-error",
   "metadata": {},
   "source": [
    "## model building: size of 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "integrated-cement",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_state = 10\n",
    "reward1 = 4\n",
    "reward2 = 2\n",
    "fire_probability = 0.1\n",
    "P_10, R_10 = mdpex.forest(S=num_state, r1=reward1, r2=reward2, p=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-delaware",
   "metadata": {},
   "source": [
    "## VI: 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "artificial-shuttle",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epsilon</th>\n",
       "      <th>Policy</th>\n",
       "      <th>Iteration</th>\n",
       "      <th>Time</th>\n",
       "      <th>Reward</th>\n",
       "      <th>Value Function</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000e-01</td>\n",
       "      <td>(0, 0, 0, 0, 0, 0, 0, 0, 0, 0)</td>\n",
       "      <td>16</td>\n",
       "      <td>0.003708</td>\n",
       "      <td>2.092869</td>\n",
       "      <td>(3.772106445047769, 4.513314520588507, 5.42838...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>(0, 0, 0, 0, 0, 0, 0, 0, 0, 0)</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000641</td>\n",
       "      <td>2.123551</td>\n",
       "      <td>(3.772106445047769, 4.513314520588507, 5.42838...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>(0, 0, 0, 0, 0, 0, 0, 0, 0, 0)</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000728</td>\n",
       "      <td>2.088074</td>\n",
       "      <td>(3.772106445047769, 4.513314520588507, 5.42838...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>(0, 0, 0, 0, 0, 0, 0, 0, 0, 0)</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000641</td>\n",
       "      <td>2.115200</td>\n",
       "      <td>(3.772106445047769, 4.513314520588507, 5.42838...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>(0, 0, 0, 0, 0, 0, 0, 0, 0, 0)</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000830</td>\n",
       "      <td>2.136778</td>\n",
       "      <td>(3.772106445047769, 4.513314520588507, 5.42838...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>(0, 0, 0, 0, 0, 0, 0, 0, 0, 0)</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000774</td>\n",
       "      <td>2.099530</td>\n",
       "      <td>(3.772106445047769, 4.513314520588507, 5.42838...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.000000e-12</td>\n",
       "      <td>(0, 0, 0, 0, 0, 0, 0, 0, 0, 0)</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>2.130398</td>\n",
       "      <td>(3.772106445047769, 4.513314520588507, 5.42838...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.000000e-14</td>\n",
       "      <td>(0, 0, 0, 0, 0, 0, 0, 0, 0, 0)</td>\n",
       "      <td>170</td>\n",
       "      <td>0.006870</td>\n",
       "      <td>2.165125</td>\n",
       "      <td>(6.0037852114428425, 6.744993286983577, 7.6600...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.000000e-16</td>\n",
       "      <td>(0, 0, 0, 0, 0, 0, 0, 0, 0, 0)</td>\n",
       "      <td>192</td>\n",
       "      <td>0.008128</td>\n",
       "      <td>2.108321</td>\n",
       "      <td>(6.003785392141506, 6.744993467682242, 7.66006...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Epsilon                          Policy Iteration      Time    Reward  \\\n",
       "0  1.000000e-01  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0)        16  0.003708  2.092869   \n",
       "1  1.000000e-02  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0)        16  0.000641  2.123551   \n",
       "2  1.000000e-04  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0)        16  0.000728  2.088074   \n",
       "3  1.000000e-06  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0)        16  0.000641  2.115200   \n",
       "4  1.000000e-08  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0)        16  0.000830  2.136778   \n",
       "5  1.000000e-10  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0)        16  0.000774  2.099530   \n",
       "6  1.000000e-12  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0)        16  0.000670  2.130398   \n",
       "7  1.000000e-14  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0)       170  0.006870  2.165125   \n",
       "8  1.000000e-16  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0)       192  0.008128  2.108321   \n",
       "\n",
       "                                      Value Function  \n",
       "0  (3.772106445047769, 4.513314520588507, 5.42838...  \n",
       "1  (3.772106445047769, 4.513314520588507, 5.42838...  \n",
       "2  (3.772106445047769, 4.513314520588507, 5.42838...  \n",
       "3  (3.772106445047769, 4.513314520588507, 5.42838...  \n",
       "4  (3.772106445047769, 4.513314520588507, 5.42838...  \n",
       "5  (3.772106445047769, 4.513314520588507, 5.42838...  \n",
       "6  (3.772106445047769, 4.513314520588507, 5.42838...  \n",
       "7  (6.0037852114428425, 6.744993286983577, 7.6600...  \n",
       "8  (6.003785392141506, 6.744993467682242, 7.66006...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vi_10_result = trainVI(P_10, R_10, epsilon=[1e-1, 1e-2, 1e-4, 1e-6, 1e-8, 1e-10, 1e-12, 1e-14, 1e-16])\n",
    "vi_10_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-doctrine",
   "metadata": {},
   "source": [
    "## PI:10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fourth-acoustic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.99: {10000.0: {'policy': (0, 0, 0, 0, 0, 0, 0, 0, 0, 0),\n",
       "   'reward': 2.091547045081238,\n",
       "   'iteration': 9,\n",
       "   'time': 0.00975489616394043},\n",
       "  100000.0: {'policy': (0, 0, 0, 0, 0, 0, 0, 0, 0, 0),\n",
       "   'reward': 2.1633066396664224,\n",
       "   'iteration': 9,\n",
       "   'time': 0.002244234085083008},\n",
       "  1000000.0: {'policy': (0, 0, 0, 0, 0, 0, 0, 0, 0, 0),\n",
       "   'reward': 2.0790002298047154,\n",
       "   'iteration': 9,\n",
       "   'time': 0.0024559497833251953}},\n",
       " 0.9: {10000.0: {'policy': (0, 0, 0, 0, 0, 0, 0, 0, 0, 0),\n",
       "   'reward': 2.1004140803238083,\n",
       "   'iteration': 9,\n",
       "   'time': 0.0030319690704345703},\n",
       "  100000.0: {'policy': (0, 0, 0, 0, 0, 0, 0, 0, 0, 0),\n",
       "   'reward': 2.110752373589253,\n",
       "   'iteration': 9,\n",
       "   'time': 0.002566814422607422},\n",
       "  1000000.0: {'policy': (0, 0, 0, 0, 0, 0, 0, 0, 0, 0),\n",
       "   'reward': 2.0498449287300913,\n",
       "   'iteration': 9,\n",
       "   'time': 0.0030012130737304688}},\n",
       " 0.8: {10000.0: {'policy': (0, 1, 1, 1, 0, 0, 0, 0, 0, 0),\n",
       "   'reward': 2.353651793240591,\n",
       "   'iteration': 6,\n",
       "   'time': 0.0022041797637939453},\n",
       "  100000.0: {'policy': (0, 1, 1, 1, 0, 0, 0, 0, 0, 0),\n",
       "   'reward': 2.400241435542717,\n",
       "   'iteration': 6,\n",
       "   'time': 0.0021619796752929688},\n",
       "  1000000.0: {'policy': (0, 1, 1, 1, 0, 0, 0, 0, 0, 0),\n",
       "   'reward': 2.3914023251616365,\n",
       "   'iteration': 6,\n",
       "   'time': 0.002318143844604492}},\n",
       " 0.7: {10000.0: {'policy': (0, 1, 1, 1, 1, 0, 0, 0, 0, 0),\n",
       "   'reward': 2.4665019026805175,\n",
       "   'iteration': 5,\n",
       "   'time': 0.0017790794372558594},\n",
       "  100000.0: {'policy': (0, 1, 1, 1, 1, 0, 0, 0, 0, 0),\n",
       "   'reward': 2.4998446350536097,\n",
       "   'iteration': 5,\n",
       "   'time': 0.0014231204986572266},\n",
       "  1000000.0: {'policy': (0, 1, 1, 1, 1, 0, 0, 0, 0, 0),\n",
       "   'reward': 2.5562354027390013,\n",
       "   'iteration': 5,\n",
       "   'time': 0.001528024673461914}},\n",
       " 0.6: {10000.0: {'policy': (0, 1, 1, 1, 1, 1, 1, 0, 0, 0),\n",
       "   'reward': 2.6671977052797073,\n",
       "   'iteration': 3,\n",
       "   'time': 0.0011191368103027344},\n",
       "  100000.0: {'policy': (0, 1, 1, 1, 1, 1, 1, 0, 0, 0),\n",
       "   'reward': 2.7025783347065153,\n",
       "   'iteration': 3,\n",
       "   'time': 0.0012171268463134766},\n",
       "  1000000.0: {'policy': (0, 1, 1, 1, 1, 1, 1, 0, 0, 0),\n",
       "   'reward': 2.692345083788877,\n",
       "   'iteration': 3,\n",
       "   'time': 0.001107931137084961}},\n",
       " 0.5: {10000.0: {'policy': (0, 1, 1, 1, 1, 1, 1, 0, 0, 0),\n",
       "   'reward': 2.7018966156651016,\n",
       "   'iteration': 3,\n",
       "   'time': 0.0013041496276855469},\n",
       "  100000.0: {'policy': (0, 1, 1, 1, 1, 1, 1, 0, 0, 0),\n",
       "   'reward': 2.679217477255199,\n",
       "   'iteration': 3,\n",
       "   'time': 0.0010998249053955078},\n",
       "  1000000.0: {'policy': (0, 1, 1, 1, 1, 1, 1, 0, 0, 0),\n",
       "   'reward': 2.6775809811364124,\n",
       "   'iteration': 3,\n",
       "   'time': 0.0013289451599121094}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_10_result = train_pi(P_10, R_10, gamma = [0.99, 0.9, 0.8, 0.7, 0.6, 0.5], max_iter = [1e4, 1e5, 1e6])\n",
    "pi_10_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-socket",
   "metadata": {},
   "source": [
    "## QL: 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "herbal-telephone",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_decs = [0.9, 0.99, 0.999]\n",
    "alpha_mins =[0.001, 0.0001]\n",
    "eps = [10.0, 1.0]\n",
    "eps_dec = [0.9, 0.99, 0.999]\n",
    "iters = [1e6, 1e7, 1e8]\n",
    "ql_10_result = trainQ(P_10, R_10, discount=0.9, alpha_dec=alpha_decs, alpha_min=alpha_mins, \n",
    "            epsilon=eps, epsilon_decay=eps_dec, n_iter=iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-guidance",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:CS6476] *",
   "language": "python",
   "name": "conda-env-CS6476-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
