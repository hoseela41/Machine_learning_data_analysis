{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "given-decision",
   "metadata": {},
   "source": [
    "# Module loading and grid initiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "drawn-soundtrack",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import seaborn\n",
    "import matplotlib.pylab as plt\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "accompanied-lawyer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym.envs.toy_text.frozen_lake as fl\n",
    "np.random.seed(56) # set random seed\n",
    "map_eight = fl.generate_random_map(size = 8, p = 0.8)\n",
    "map_twenty_five = fl.generate_random_map(size = 25, p = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "realistic-waters",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SFFFFHFF', 'HFFFHFFF', 'HFFHFFFF', 'FFFFHHFH', 'FFHFFFFF', 'FFFFFFFF', 'FFHFHFFF', 'FFFFFFFG']\n"
     ]
    }
   ],
   "source": [
    "print(map_eight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-brunei",
   "metadata": {},
   "source": [
    "# Functions defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "rotary-sleeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_policy(env, policy, n_epoch=1000):\n",
    "    rewards = []\n",
    "    episodes = []\n",
    "    for i in range(n_epoch):\n",
    "        initial_state = env.reset()\n",
    "        episode = 0\n",
    "        done = False\n",
    "        total_reward =0;\n",
    "        # calculate until ep converge\n",
    "        while not done and episode < n_epoch:\n",
    "            episode += 1\n",
    "            action = int(policy[initial_state])\n",
    "            new_s, r, done, _ = env.step(action)\n",
    "            total_reward += r\n",
    "            initial_state = new_s\n",
    "        rewards.append(total_reward)\n",
    "        episodes.append(episode)\n",
    "    \n",
    "    mean_r = sum(rewards) / len(rewards)\n",
    "    mean_e = sum(episodes) / len(episodes)\n",
    "    return mean_r, mean_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "respected-scanning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, discount=0.8, epsilon=1e-10):\n",
    "    # initiate the variables\n",
    "    start_time = time.time()\n",
    "    num_states = env.observation_space.n\n",
    "    num_actions = env.action_space.n\n",
    "    policies = np.zeros(shape = (1, num_states)) # policies should be as same as states\n",
    "    values = np.zeros(shape = (1, num_states))\n",
    "    prev_values = np.copy(values)\n",
    "    sigma = discount\n",
    "    iteration = 0\n",
    "    diff = float(\"inf\")\n",
    "    # do calculation until the max of diff between old and new values < epsilon\n",
    "    while diff > epsilon:\n",
    "        iteration += 1\n",
    "        # go through all states\n",
    "        for s in range(num_states):\n",
    "            best_value = float(\"-inf\")\n",
    "            # check all actions\n",
    "            for a in range(num_actions):\n",
    "                # calculate the possible value from all actions\n",
    "                total_value = 0\n",
    "                for prob, new_s, r, done in env.P[s][a]:\n",
    "                    value_new_s = prev_values[0][new_s]\n",
    "                    cand_value = None\n",
    "                    if done:\n",
    "                        cand_value = r\n",
    "                    else:\n",
    "                        cand_value = r + sigma * value_new_s\n",
    "                    total_value += prob * cand_value\n",
    "                # update the new one if the totaal value is better than the current best one\n",
    "                if total_value > best_value:\n",
    "                    # update \n",
    "                    best_value = total_value\n",
    "                    policies[0][s] = a\n",
    "                    values[0][s] = best_value\n",
    "                    \n",
    "        # compare the updated values with prev value list\n",
    "        diff_list = np.abs(values - prev_values)\n",
    "        diff = np.max(diff_list)\n",
    "        prev_values = np.copy(values)\n",
    "\n",
    "    end_time = time.time()\n",
    "    runtime = end_time - start_time\n",
    "    \n",
    "    return policies[0], iteration, runtime\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "dependent-jones",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, discount=0.8, epsilon=1e-10):\n",
    "    start_time = time.time()\n",
    "    num_states = env.observation_space.n\n",
    "    num_actions = env.action_space.n\n",
    "    # generate random polocies\n",
    "    policies = np.random.randint(num_actions, size = (1, num_states))\n",
    "    values = np.zeros((1, num_states))\n",
    "    iterations = 0\n",
    "    sigma = discount \n",
    "    converge = False\n",
    "    while not converge:\n",
    "        # 1. calculate the values from valuation iterations\n",
    "        iterations += 1\n",
    "        reach_threshold = False\n",
    "        while not reach_threshold:\n",
    "            diff = 0\n",
    "            for s in range(num_states):\n",
    "                val = values[0][s]\n",
    "                act = policies[0][s]\n",
    "                total_values = 0\n",
    "                for prob, new_s, r, done in env.P[s][act]:\n",
    "                    value_new_s = values[0][new_s]\n",
    "                    # calculate reward\n",
    "                    state_val = None\n",
    "                    if done:\n",
    "                        state_val = r\n",
    "                    else:\n",
    "                        state_val = r + sigma * value_new_s\n",
    "                    total_values += state_val * prob\n",
    "                    \n",
    "                values[0][s] = total_values\n",
    "                diff = max(diff, np.abs(val - values[0][s]))\n",
    "        \n",
    "            if diff < epsilon:\n",
    "                reach_threshold = True\n",
    "         \n",
    "        # 2. compare the old and new policy\n",
    "        converge = True\n",
    "        for s in range(num_states):\n",
    "            prev_action = policies[0][s]\n",
    "            max_value = float(\"-inf\")\n",
    "            for a in range(num_actions):\n",
    "                total_values = 0\n",
    "                for prob, new_s, r, done in env.P[s][a]:\n",
    "                    value_new_s = values[0][new_s]\n",
    "                    state_val = None\n",
    "                    if done:\n",
    "                        state_val = r\n",
    "                    else:\n",
    "                        state_val = r + sigma * value_new_s\n",
    "                    total_values += state_val * prob\n",
    "                if total_values > max_value:\n",
    "                    max_value = total_values\n",
    "                    policies[0][s] = a\n",
    "            \n",
    "            if prev_action != policies[0][s]:\n",
    "                converge = False\n",
    "    end_time = time.time()\n",
    "    runtime = end_time - start_time\n",
    "    return policies[0], iterations, runtime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "potential-colon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(env, algo = \"Value Iteration\", discount = [0.1, 0.2], epsilon = [1e-8, 1e-9], info = False):\n",
    "    res = {}\n",
    "    for disc in discount:\n",
    "        res[disc] = {} # create a dict inside dic\n",
    "        for ep in epsilon:\n",
    "            res[disc][ep] = {}\n",
    "            policy, iteration, runtime = None, None, None\n",
    "            if algo == \"Value Iteration\":\n",
    "                policy, iteration, runtime = value_iteration(env, disc, ep)\n",
    "            elif algo == \"Policy Iteration\":\n",
    "                policy, iteration, runtime = policy_iteration(env, disc, ep)\n",
    "            mean_reward, mean_epsilon = test_policy(env, policy)\n",
    "            res[disc][ep][\"mean_reward\"] = mean_reward\n",
    "            res[disc][ep][\"mean_epsilon\"] = mean_epsilon\n",
    "            res[disc][ep][\"policy\"] = policy\n",
    "            res[disc][ep][\"iteration\"] = iteration\n",
    "            res[disc][ep][\"runtime\"] = runtime\n",
    "            if info is True:\n",
    "                print(\"==== {}: discount - {} and epsilon - {}====\".format(algo, disc, ep))\n",
    "                print(\"runtime is {} second, {} iterations, the mean rewards:{} and epsilons: {}\".format(runtime, iteration, mean_reward, mean_epsilon))\n",
    "            \n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-ecology",
   "metadata": {},
   "source": [
    "## Eight by eight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "similar-notion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Value Iteration: discount - 0.5 and epsilon - 0.001====\n",
      "runtime is 0.011658906936645508 second, 8 iterations, the mean rewards:0.0 and epsilons: 3.05\n",
      "==== Value Iteration: discount - 0.5 and epsilon - 1e-06====\n",
      "runtime is 0.013071775436401367 second, 17 iterations, the mean rewards:0.552 and epsilons: 64.869\n",
      "==== Value Iteration: discount - 0.5 and epsilon - 1e-09====\n",
      "runtime is 0.051757097244262695 second, 25 iterations, the mean rewards:0.568 and epsilons: 66.084\n",
      "==== Value Iteration: discount - 0.5 and epsilon - 1e-12====\n",
      "runtime is 0.048452138900756836 second, 35 iterations, the mean rewards:0.613 and epsilons: 69.703\n",
      "==== Value Iteration: discount - 0.5 and epsilon - 1e-15====\n",
      "runtime is 0.04793596267700195 second, 45 iterations, the mean rewards:0.592 and epsilons: 67.036\n",
      "==== Value Iteration: discount - 0.75 and epsilon - 0.001====\n",
      "runtime is 0.009991884231567383 second, 14 iterations, the mean rewards:0.296 and epsilons: 34.599\n",
      "==== Value Iteration: discount - 0.75 and epsilon - 1e-06====\n",
      "runtime is 0.02924513816833496 second, 35 iterations, the mean rewards:0.587 and epsilons: 68.456\n",
      "==== Value Iteration: discount - 0.75 and epsilon - 1e-09====\n",
      "runtime is 0.04360699653625488 second, 57 iterations, the mean rewards:0.595 and epsilons: 67.088\n",
      "==== Value Iteration: discount - 0.75 and epsilon - 1e-12====\n",
      "runtime is 0.06749677658081055 second, 80 iterations, the mean rewards:0.582 and epsilons: 66.2\n",
      "==== Value Iteration: discount - 0.75 and epsilon - 1e-15====\n",
      "runtime is 0.0783839225769043 second, 102 iterations, the mean rewards:0.587 and epsilons: 65.756\n",
      "==== Value Iteration: discount - 0.9 and epsilon - 0.001====\n",
      "runtime is 0.028559207916259766 second, 28 iterations, the mean rewards:0.585 and epsilons: 67.51\n",
      "==== Value Iteration: discount - 0.9 and epsilon - 1e-06====\n",
      "runtime is 0.08868789672851562 second, 83 iterations, the mean rewards:0.596 and epsilons: 66.945\n",
      "==== Value Iteration: discount - 0.9 and epsilon - 1e-09====\n",
      "runtime is 0.11676383018493652 second, 138 iterations, the mean rewards:0.607 and epsilons: 67.3\n",
      "==== Value Iteration: discount - 0.9 and epsilon - 1e-12====\n",
      "runtime is 0.15692806243896484 second, 191 iterations, the mean rewards:0.598 and epsilons: 66.095\n",
      "==== Value Iteration: discount - 0.9 and epsilon - 1e-15====\n",
      "runtime is 0.2327258586883545 second, 244 iterations, the mean rewards:0.589 and epsilons: 66.686\n",
      "==== Value Iteration: discount - 0.95 and epsilon - 0.001====\n",
      "runtime is 0.038308143615722656 second, 49 iterations, the mean rewards:0.586 and epsilons: 66.801\n",
      "==== Value Iteration: discount - 0.95 and epsilon - 1e-06====\n",
      "runtime is 0.11759471893310547 second, 145 iterations, the mean rewards:0.56 and epsilons: 66.161\n",
      "==== Value Iteration: discount - 0.95 and epsilon - 1e-09====\n",
      "runtime is 0.19308686256408691 second, 235 iterations, the mean rewards:0.589 and epsilons: 67.583\n",
      "==== Value Iteration: discount - 0.95 and epsilon - 1e-12====\n",
      "runtime is 0.27789807319641113 second, 324 iterations, the mean rewards:0.587 and epsilons: 68.457\n",
      "==== Value Iteration: discount - 0.95 and epsilon - 1e-15====\n",
      "runtime is 0.3536992073059082 second, 413 iterations, the mean rewards:0.575 and epsilons: 68.505\n",
      "==== Value Iteration: discount - 0.99 and epsilon - 0.001====\n",
      "runtime is 0.12052011489868164 second, 131 iterations, the mean rewards:0.795 and epsilons: 91.88\n",
      "==== Value Iteration: discount - 0.99 and epsilon - 1e-06====\n",
      "runtime is 0.3015906810760498 second, 344 iterations, the mean rewards:0.79 and epsilons: 91.517\n",
      "==== Value Iteration: discount - 0.99 and epsilon - 1e-09====\n",
      "runtime is 0.43958497047424316 second, 541 iterations, the mean rewards:0.788 and epsilons: 89.483\n",
      "==== Value Iteration: discount - 0.99 and epsilon - 1e-12====\n",
      "runtime is 0.60746169090271 second, 738 iterations, the mean rewards:0.791 and epsilons: 93.014\n",
      "==== Value Iteration: discount - 0.99 and epsilon - 1e-15====\n",
      "runtime is 0.7669270038604736 second, 932 iterations, the mean rewards:0.788 and epsilons: 90.459\n",
      "==== Value Iteration: discount - 0.9999 and epsilon - 0.001====\n",
      "runtime is 0.16421914100646973 second, 205 iterations, the mean rewards:0.824 and epsilons: 127.015\n",
      "==== Value Iteration: discount - 0.9999 and epsilon - 1e-06====\n",
      "runtime is 0.6132941246032715 second, 664 iterations, the mean rewards:0.822 and epsilons: 129.443\n",
      "==== Value Iteration: discount - 0.9999 and epsilon - 1e-09====\n",
      "runtime is 0.9394948482513428 second, 1131 iterations, the mean rewards:0.814 and epsilons: 123.339\n",
      "==== Value Iteration: discount - 0.9999 and epsilon - 1e-12====\n",
      "runtime is 1.327568769454956 second, 1599 iterations, the mean rewards:0.824 and epsilons: 124.651\n",
      "==== Value Iteration: discount - 0.9999 and epsilon - 1e-15====\n",
      "runtime is 1.7409367561340332 second, 2062 iterations, the mean rewards:0.836 and epsilons: 128.114\n",
      "==== Policy Iteration: discount - 0.5 and epsilon - 0.001====\n",
      "runtime is 0.016729116439819336 second, 10 iterations, the mean rewards:0.589 and epsilons: 69.05\n",
      "==== Policy Iteration: discount - 0.5 and epsilon - 1e-06====\n",
      "runtime is 0.021738052368164062 second, 4 iterations, the mean rewards:0.593 and epsilons: 66.048\n",
      "==== Policy Iteration: discount - 0.5 and epsilon - 1e-09====\n",
      "runtime is 0.032593727111816406 second, 5 iterations, the mean rewards:0.57 and epsilons: 65.365\n",
      "==== Policy Iteration: discount - 0.5 and epsilon - 1e-12====\n",
      "runtime is 0.05913901329040527 second, 5 iterations, the mean rewards:0.576 and epsilons: 64.965\n",
      "==== Policy Iteration: discount - 0.5 and epsilon - 1e-15====\n",
      "runtime is 0.08074116706848145 second, 6 iterations, the mean rewards:0.576 and epsilons: 68.739\n",
      "==== Policy Iteration: discount - 0.75 and epsilon - 0.001====\n",
      "runtime is 0.023274898529052734 second, 10 iterations, the mean rewards:0.572 and epsilons: 66.939\n",
      "==== Policy Iteration: discount - 0.75 and epsilon - 1e-06====\n",
      "runtime is 0.0373539924621582 second, 3 iterations, the mean rewards:0.593 and epsilons: 67.853\n",
      "==== Policy Iteration: discount - 0.75 and epsilon - 1e-09====\n",
      "runtime is 0.1035318374633789 second, 5 iterations, the mean rewards:0.572 and epsilons: 65.816\n",
      "==== Policy Iteration: discount - 0.75 and epsilon - 1e-12====\n",
      "runtime is 0.18619632720947266 second, 4 iterations, the mean rewards:0.58 and epsilons: 66.846\n",
      "==== Policy Iteration: discount - 0.75 and epsilon - 1e-15====\n",
      "runtime is 0.20554399490356445 second, 6 iterations, the mean rewards:0.568 and epsilons: 64.223\n",
      "==== Policy Iteration: discount - 0.9 and epsilon - 0.001====\n",
      "runtime is 0.09112405776977539 second, 5 iterations, the mean rewards:0.604 and epsilons: 66.834\n",
      "==== Policy Iteration: discount - 0.9 and epsilon - 1e-06====\n",
      "runtime is 0.1066899299621582 second, 5 iterations, the mean rewards:0.542 and epsilons: 62.968\n",
      "==== Policy Iteration: discount - 0.9 and epsilon - 1e-09====\n",
      "runtime is 0.1956038475036621 second, 5 iterations, the mean rewards:0.567 and epsilons: 67.373\n",
      "==== Policy Iteration: discount - 0.9 and epsilon - 1e-12====\n",
      "runtime is 0.3428490161895752 second, 5 iterations, the mean rewards:0.596 and epsilons: 67.797\n",
      "==== Policy Iteration: discount - 0.9 and epsilon - 1e-15====\n",
      "runtime is 0.33263731002807617 second, 5 iterations, the mean rewards:0.614 and epsilons: 66.36\n",
      "==== Policy Iteration: discount - 0.95 and epsilon - 0.001====\n",
      "runtime is 0.046485185623168945 second, 5 iterations, the mean rewards:0.597 and epsilons: 67.227\n",
      "==== Policy Iteration: discount - 0.95 and epsilon - 1e-06====\n",
      "runtime is 0.15598201751708984 second, 8 iterations, the mean rewards:0.606 and epsilons: 67.659\n",
      "==== Policy Iteration: discount - 0.95 and epsilon - 1e-09====\n",
      "runtime is 0.43974876403808594 second, 9 iterations, the mean rewards:0.586 and epsilons: 67.322\n",
      "==== Policy Iteration: discount - 0.95 and epsilon - 1e-12====\n",
      "runtime is 0.3465700149536133 second, 4 iterations, the mean rewards:0.605 and epsilons: 67.42\n",
      "==== Policy Iteration: discount - 0.95 and epsilon - 1e-15====\n",
      "runtime is 0.4762990474700928 second, 4 iterations, the mean rewards:0.574 and epsilons: 65.876\n",
      "==== Policy Iteration: discount - 0.99 and epsilon - 0.001====\n",
      "runtime is 0.14014482498168945 second, 10 iterations, the mean rewards:0.826 and epsilons: 92.725\n",
      "==== Policy Iteration: discount - 0.99 and epsilon - 1e-06====\n",
      "runtime is 0.3886258602142334 second, 8 iterations, the mean rewards:0.762 and epsilons: 89.359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Policy Iteration: discount - 0.99 and epsilon - 1e-09====\n",
      "runtime is 0.8630111217498779 second, 7 iterations, the mean rewards:0.807 and epsilons: 89.919\n",
      "==== Policy Iteration: discount - 0.99 and epsilon - 1e-12====\n",
      "runtime is 1.3038158416748047 second, 6 iterations, the mean rewards:0.802 and epsilons: 93.199\n",
      "==== Policy Iteration: discount - 0.99 and epsilon - 1e-15====\n",
      "runtime is 1.4689629077911377 second, 9 iterations, the mean rewards:0.792 and epsilons: 93.329\n",
      "==== Policy Iteration: discount - 0.9999 and epsilon - 0.001====\n",
      "runtime is 0.2861001491546631 second, 59 iterations, the mean rewards:0.822 and epsilons: 123.637\n",
      "==== Policy Iteration: discount - 0.9999 and epsilon - 1e-06====\n",
      "runtime is 0.7969357967376709 second, 8 iterations, the mean rewards:0.812 and epsilons: 126.799\n",
      "==== Policy Iteration: discount - 0.9999 and epsilon - 1e-09====\n",
      "runtime is 1.5573959350585938 second, 5 iterations, the mean rewards:0.816 and epsilons: 124.881\n",
      "==== Policy Iteration: discount - 0.9999 and epsilon - 1e-12====\n",
      "runtime is 1.940173625946045 second, 6 iterations, the mean rewards:0.848 and epsilons: 129.987\n",
      "==== Policy Iteration: discount - 0.9999 and epsilon - 1e-15====\n",
      "runtime is 2.3890299797058105 second, 8 iterations, the mean rewards:0.805 and epsilons: 130.783\n"
     ]
    }
   ],
   "source": [
    "fl_8x8 = FrozenLakeEnv(desc=map_eight)\n",
    "# fl_8x8 = gym.make(\"FrozenLake-v0\")\n",
    "discount_list = [0.5, 0.75, 0.9, 0.95, 0.99, 0.9999]\n",
    "epsilon_list = [1e-3, 1e-6, 1e-9, 1e-12, 1e-15]\n",
    "vi_result = train_test(fl_8x8, \"Value Iteration\", discount= discount_list, \n",
    "                                        epsilon= epsilon_list, \n",
    "                                        info = True)\n",
    "pi_result = train_test(fl_8x8, \"Policy Iteration\", discount=discount_list, \n",
    "                                        epsilon=epsilon_list, \n",
    "                                        info = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "supreme-capital",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.001: {'mean_reward': 0.795,\n",
       "  'mean_epsilon': 91.88,\n",
       "  'policy': array([3., 2., 0., 0., 0., 0., 2., 0., 0., 2., 0., 0., 0., 1., 2., 1., 0.,\n",
       "         2., 0., 0., 1., 3., 2., 3., 1., 0., 3., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 2., 1., 1., 1., 1., 1., 0., 1., 2., 3., 2., 1., 1., 1., 0., 0.,\n",
       "         0., 0., 2., 1., 1., 1., 1., 1., 1., 1., 2., 2., 0.]),\n",
       "  'iteration': 131,\n",
       "  'runtime': 0.12052011489868164},\n",
       " 1e-06: {'mean_reward': 0.79,\n",
       "  'mean_epsilon': 91.517,\n",
       "  'policy': array([3., 2., 0., 0., 0., 0., 2., 0., 0., 2., 0., 0., 0., 1., 2., 1., 0.,\n",
       "         2., 0., 0., 1., 3., 2., 3., 1., 0., 3., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 2., 1., 1., 1., 1., 1., 0., 1., 2., 3., 2., 1., 1., 1., 0., 0.,\n",
       "         0., 0., 2., 1., 1., 1., 1., 1., 1., 1., 2., 2., 0.]),\n",
       "  'iteration': 344,\n",
       "  'runtime': 0.3015906810760498},\n",
       " 1e-09: {'mean_reward': 0.788,\n",
       "  'mean_epsilon': 89.483,\n",
       "  'policy': array([3., 2., 0., 0., 0., 0., 2., 0., 0., 2., 0., 0., 0., 1., 2., 1., 0.,\n",
       "         2., 0., 0., 1., 3., 2., 3., 1., 0., 3., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 2., 1., 1., 1., 1., 1., 0., 1., 2., 3., 2., 1., 1., 1., 0., 0.,\n",
       "         0., 0., 2., 1., 1., 1., 1., 1., 1., 1., 2., 2., 0.]),\n",
       "  'iteration': 541,\n",
       "  'runtime': 0.43958497047424316},\n",
       " 1e-12: {'mean_reward': 0.791,\n",
       "  'mean_epsilon': 93.014,\n",
       "  'policy': array([3., 2., 0., 0., 0., 0., 2., 0., 0., 2., 0., 0., 0., 1., 2., 1., 0.,\n",
       "         2., 0., 0., 1., 3., 2., 3., 1., 0., 3., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 2., 1., 1., 1., 1., 1., 0., 1., 2., 3., 2., 1., 1., 1., 0., 0.,\n",
       "         0., 0., 2., 1., 1., 1., 1., 1., 1., 1., 2., 2., 0.]),\n",
       "  'iteration': 738,\n",
       "  'runtime': 0.60746169090271},\n",
       " 1e-15: {'mean_reward': 0.788,\n",
       "  'mean_epsilon': 90.459,\n",
       "  'policy': array([3., 2., 0., 0., 0., 0., 2., 1., 0., 2., 0., 0., 0., 1., 2., 1., 0.,\n",
       "         2., 0., 0., 1., 3., 2., 3., 1., 0., 3., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 2., 1., 1., 1., 1., 1., 0., 1., 2., 3., 2., 1., 1., 1., 0., 0.,\n",
       "         0., 0., 2., 1., 1., 1., 1., 1., 1., 1., 2., 2., 0.]),\n",
       "  'iteration': 932,\n",
       "  'runtime': 0.7669270038604736}}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vi_result[0.99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "aquatic-choice",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.001: {'mean_reward': 0.826,\n",
       "  'mean_epsilon': 92.725,\n",
       "  'policy': array([3, 2, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 1, 2, 1, 0, 2, 0, 0, 1, 3,\n",
       "         2, 3, 1, 0, 3, 0, 0, 0, 0, 0, 1, 0, 0, 2, 1, 1, 1, 1, 1, 0, 1, 2,\n",
       "         3, 2, 1, 1, 1, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 0]),\n",
       "  'iteration': 10,\n",
       "  'runtime': 0.14014482498168945},\n",
       " 1e-06: {'mean_reward': 0.762,\n",
       "  'mean_epsilon': 89.359,\n",
       "  'policy': array([3, 2, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 1, 2, 1, 0, 2, 0, 0, 1, 3,\n",
       "         2, 3, 1, 0, 3, 0, 0, 0, 0, 0, 1, 0, 0, 2, 1, 1, 1, 1, 1, 0, 1, 2,\n",
       "         3, 2, 1, 1, 1, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 0]),\n",
       "  'iteration': 8,\n",
       "  'runtime': 0.3886258602142334},\n",
       " 1e-09: {'mean_reward': 0.807,\n",
       "  'mean_epsilon': 89.919,\n",
       "  'policy': array([3, 2, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 1, 2, 1, 0, 2, 0, 0, 1, 3,\n",
       "         2, 3, 1, 0, 3, 0, 0, 0, 0, 0, 1, 0, 0, 2, 1, 1, 1, 1, 1, 0, 1, 2,\n",
       "         3, 2, 1, 1, 1, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 0]),\n",
       "  'iteration': 7,\n",
       "  'runtime': 0.8630111217498779},\n",
       " 1e-12: {'mean_reward': 0.802,\n",
       "  'mean_epsilon': 93.199,\n",
       "  'policy': array([3, 2, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 1, 2, 1, 0, 2, 0, 0, 1, 3,\n",
       "         2, 3, 1, 0, 3, 0, 0, 0, 0, 0, 1, 0, 0, 2, 1, 1, 1, 1, 1, 0, 1, 2,\n",
       "         3, 2, 1, 1, 1, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 0]),\n",
       "  'iteration': 6,\n",
       "  'runtime': 1.3038158416748047},\n",
       " 1e-15: {'mean_reward': 0.792,\n",
       "  'mean_epsilon': 93.329,\n",
       "  'policy': array([3, 2, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 1, 2, 1, 0, 2, 0, 0, 1, 3,\n",
       "         2, 3, 1, 0, 3, 0, 0, 0, 0, 0, 1, 0, 0, 2, 1, 1, 1, 1, 1, 0, 1, 2,\n",
       "         3, 2, 1, 1, 1, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 0]),\n",
       "  'iteration': 9,\n",
       "  'runtime': 1.4689629077911377}}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_result[0.99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "different-drive",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def convert_dict_to_dict(the_dict):\n",
    "    \n",
    "    # return for discount\n",
    "    discount_rewards = {}\n",
    "    discount_iterations = {}\n",
    "    discount_times = {}\n",
    "\n",
    "\n",
    "    for disc in the_dict:\n",
    "        discount_rewards[disc] = []    \n",
    "        discount_iterations[disc] = []    \n",
    "        discount_times[disc] = []\n",
    "\n",
    "        for eps in the_dict[disc]:\n",
    "            discount_rewards[disc].append(the_dict[disc][eps]['mean_reward'])\n",
    "            discount_iterations[disc].append(the_dict[disc][eps]['iteration'])        \n",
    "            discount_times[disc].append(the_dict[disc][eps]['time_spent'].total_seconds())  \n",
    "\n",
    "            \n",
    "    epsilon_rewards = {}\n",
    "    epsilon_iterations = {}\n",
    "    epsilon_times = {}\n",
    "    for eps in the_dict[0.5]:\n",
    "        epsilon_rewards[eps] = []    \n",
    "        epsilon_iterations[eps] = []    \n",
    "        epsilon_times[eps] = []\n",
    "    \n",
    "        for disc in vi_dict:\n",
    "            epsilon_rewards[eps].append(the_dict[disc][eps]['mean_reward'])\n",
    "            epsilon_iterations[eps].append(the_dict[disc][eps]['iteration'])        \n",
    "            epsilon_times[eps].append(the_dict[disc][eps]['time_spent'].total_seconds()) \n",
    "            \n",
    "    return discount_rewards, discount_iterations, discount_times, epsilon_rewards, epsilon_iterations, epsilon_times\n",
    "\n",
    "\n",
    "def see_policy(map_size, policy):\n",
    "    map_name = str(map_size)+\"x\"+str(map_size)\n",
    "    data = map_discretize(MAPS[map_name])\n",
    "    np_pol = policy_numpy(policy)\n",
    "    plt.imshow(data, interpolation=\"nearest\")\n",
    "\n",
    "    for i in range(np_pol[0].size):\n",
    "        for j in range(np_pol[0].size):\n",
    "            arrow = '\\u2190'\n",
    "            if np_pol[i, j] == 1:\n",
    "                arrow = '\\u2193'\n",
    "            elif np_pol[i, j] == 2:\n",
    "                arrow = '\\u2192'\n",
    "            elif np_pol[i, j] == 3:\n",
    "                arrow = '\\u2191'\n",
    "            text = plt.text(j, i, arrow,\n",
    "                           ha=\"center\", va=\"center\", color=\"w\")\n",
    "    plt.show()\n",
    "\n",
    "pol = vi_result[0.99][1e-12]['policy']\n",
    "vi4 = convert_dict_to_dict(vi_dict)\n",
    "see_policy(4, pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "employed-video",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:CS6476] *",
   "language": "python",
   "name": "conda-env-CS6476-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
